plus_k_sd <- round(mean_culmen_length + k*sd_culmen_length, 1)
percent <- (1 - 1/k^2)*100
cat("By Chebyshev's Rule, at least", percent, "% of the penguins' culmen lengths are between", minus_k_sd, "and", plus_k_sd, "mm.")
library(ggplot2)
library(dplyr)
library(gt)
weights_freq_distribution <- data.frame(table(df$weight))
weights <- as.numeric(as.character(weights_freq_distribution$Var1))
probabilities <- round(weights_freq_distribution$Freq/sum(weights_freq_distribution$Freq), 4)
sum(probabilities) #should sum to 1
# Construct a discrete probability distribution of weights
weights_discrete_prob_distribution <- data.frame(weights, probabilities)
# Nice print out of discrete probability distribution
weights_discrete_prob_distribution %>% gt()
# Graph of discrete probability distribution
ggplot(weights_discrete_prob_distribution, aes(x=weights, y=probabilities)) +
geom_col(fill = "steelblue", color = "black") + # Use geom_col() for bar plots of pre-summarized data
labs(title = "Discrete Probability Distribution of Penguin Weights",
x = "Weight (g)",
y = "Probability") +
theme_minimal() + # Cleaner theme
theme(plot.title = element_text(hjust = 0.5)) # Center the plot title
####Binomial Probability Distribution  for Species
#Find the probability a penguin is species Adelie (one trial)
p <- sum(df$species == "Adelie")/ length(df$species)
#Find the probability a penguin is not species Adelie (one trial)
q <- sum(df$species != "Adelie")/ length(df$species)
#Check that  p + q = 1
p + q
#Find binomial probabilities that if n penguins are selected at random,
# exactly x of them will be Adelie species
x <- 0:10
n <- 10
binomial_probabilities <- round(dbinom(x,n,p),4)
#Construct binomial probability distribution
species_binomial_prob_distribution <- data.frame(x, binomial_probabilities)
#Nice print our of binomial probability distribution
species_binomial_prob_distribution %>% gt()
breaks <- c(0:n)
ggplot(species_binomial_prob_distribution, aes(x, y=binomial_probabilities, fill="red")) + geom_bar(stat = "identity") + scale_x_continuous(breaks = breaks) + ggtitle("Binomial Probability Distribution for Species Adelie")
# Cleaner density plot visualization in place of discrete probability distribution for weights
ggplot(df, aes(x = weight)) +
geom_density(fill = "blue", alpha = 0.5) +
scale_y_continuous(labels = scales::comma) +
xlab("Weight (g)") +
ylab("Density") +
ggtitle("Density Plot of Penguin Body Weight in Grams")
### Continuous Probability Distribution for Weights
# Construct a Q-Q plot for body_mass_g
qq_plot <- ggplot(df, aes(sample = weight)) +
stat_qq(color = "red") +
stat_qq_line() +
ggtitle('Q-Q Plot of Penguin Body Mass') +
xlab('Theoretical Quantiles') +
ylab('Sample Quantiles')
# Display the Q-Q plot
print(qq_plot)
# Calculate mean and standard deviation of body_mass_g
body_mass_stats <- df %>%
summarise(mean_mass = mean(weight, na.rm = TRUE),
sd_mass = sd(weight, na.rm = TRUE))
# Extract the mean and standard deviation
mean_mass <- body_mass_stats$mean_mass
sd_mass <- body_mass_stats$sd_mass
# Density plot with superimposed normal distribution
ggplot(df, aes(x = weight)) +
geom_density(fill = "blue", alpha = 0.5) +
stat_function(fun = dnorm, args = list(mean = mean_mass, sd = sd_mass), color = "red", linetype = "dashed") +
scale_y_continuous(labels = scales::comma) +
xlab("Weight (g)") +
ylab("Density") +
ggtitle("Density Plot of Weight in Grams with Superimposed Normal Distribution")
#Normal Approximation to Binomial Distribution for Species
# Find the probability a penguin is species Adelie (one trial)
p <- sum(df$species == "Adelie") / length(df$species)
# Find the probability a penguin is not species Adelie (one trial)
q <- sum(df$species != "Adelie") / length(df$species)
# Check that p + q = 1
p + q
# Find binomial probabilities that if n penguins are selected at random,
# exactly x of them will be Adelie, where np >= 10 and nq >= 10.
n <- 30
x <- 0:n
np <- n * p
nq <- n * q
cat('np =', np, '>= 10')
cat('nq =', nq, '>= 10')
binomial_probabilities <- round(dbinom(x, n, p), 4)
# Construct binomial probability distribution
species_binomial_prob_distribution <- data.frame(x, binomial_probabilities)
# Superimpose a normal distribution
mean_binom <- n * p
sd_binom <- sqrt(n * p * q)
ggplot(species_binomial_prob_distribution, aes(x, y = binomial_probabilities)) +
geom_bar(stat = "identity", fill = "blue") +  # Make it blue
stat_function(fun = dnorm, args = list(mean = mean_binom, sd = sd_binom), color = "red") +
scale_x_continuous(breaks = 0:n) +
ggtitle("Binomial Probability Distribution for Species Adelie with Normal Approximation")
# Find binomial probabilities that if n penguins are selected at random,
# exactly x of them will be Adelie species
x <- 0:10
n <- 10
binomial_probabilities <- round(dbinom(x, n, p), 4)
# Construct binomial probability distribution
species_binomial_prob_distribution <- data.frame(x, binomial_probabilities)
breaks <- c(0:n)
mean_binom <- n * p
sd_binom <- sqrt(n * p * q)
ggplot(species_binomial_prob_distribution, aes(x, y = binomial_probabilities)) +
geom_bar(stat = "identity", fill = "blue") +
stat_function(fun = dnorm, args = list(mean = mean_binom, sd = sd_binom), color = "red") +
scale_x_continuous(breaks = breaks) +
ggtitle("Binomial Probability Distribution for Species Adelie with Normal Approximation")
#Sampling Distribution of Sample Means for weight
set.seed(666) # for reproducibility
mu <- mean(df$weight, na.rm = TRUE) #population mean
sigma <- sd(df$weight, na.rm = TRUE) #population standard deviation
# Number of samples and sample size
msm <- 1000 # number of samples means
nsm <- 30 # sample size means
# Initialize a vector to store the sample means
sample_means <- numeric(msm)
# Loop to draw samples and compute means
for (i in 1:msm) {
sample <- sample(df$weight, nsm, replace = TRUE)
sample_means[i] <- mean(sample, na.rm = TRUE)
}
expected_mu_xbar <- mu  #expected mean of sample means
expected_sigma_xbar <- sigma/sqrt(nsm) #expected standard error of sample means
actual_mu_xbar <- mean(sample_means, na.rm = TRUE)  #actual mean of sample means
actual_sigma_xbar <- sd(sample_means, na.rm = TRUE)  #actual standard error of sample means
# Create a dataframe for the sampling distribution
sampling_distribution <- data.frame(sample_means = sample_means)
# Plot the sampling distribution
ggplot(sampling_distribution, aes(x = sample_means)) +
geom_histogram(aes(y = ..density..), binwidth = 50, color = "black", fill = "blue") +
geom_density(color = "red", size = 1) +
ggtitle("Sampling Distribution of Sample Means for Weight (g)") +
xlab("Sample Mean Weight (g)") +
ylab("Density")
qqnorm(sample_means, col = "blue")
cat('The population mean weight is:', mu, 'with standard deviation', sigma,'.')
cat('The expected mean of the sample means is:', expected_mu_xbar, 'with standard error', expected_sigma_xbar,'.')
cat('The actual mean of the sample means is:', actual_mu_xbar, 'with standard error', actual_sigma_xbar,'.')
#Sampling Distribution of Sample Proportions for Species Adelie
p <- sum(df$species == "Adelie")/length(df$species) #population proportion success
q <- 1-p #population proportion failure
nsp <- 500 #sample size proportions
msp <- 10000 #number of samples proportions
sample_positions <- c( )
sample_proportions <- c( )
for(i in 1:msp){
sample_species <- sample(df$species,nsp,replace = TRUE)
sample_proportions[i] <- sum(sample_species == "Adelie")/length(sample_species)
}
expected_mu_phat <- p #expected mean of sample proportions
expected_sigma_phat <- sqrt(p*q/nsp) #expected standard error of sample proportions
actual_mu_phat <- mean(sample_proportions) #actual mean of sample proportions
actual_sigma_phat <- sd(sample_proportions) #actual standard error of sample proportions
ggplot() + geom_density(aes(sample_proportions),color = "red") +
stat_function(fun = dnorm, n = 101, args = list(mean = actual_mu_phat, sd = actual_sigma_phat),color = "blue") +
ggtitle("Sampling Distribution of Sample Proportion for Species Adelie")
qqnorm(sample_proportions, col = "blue")
cat('The population proportion of species Adelie is:', p, '.')
cat('The expected mean of the sample proportions is:', expected_mu_phat, 'with standard error', expected_sigma_phat,'.')
cat('The actual mean of the sample proportions is:', actual_mu_phat, 'with standard error', actual_sigma_phat,'.')
library(BSDA)
library(tidyverse)
library(dplyr)
#Find population mean and standard deviation
mu <- mean(df$weight, na.rm = TRUE) #population mean
sigma <- sd(df$weight, na.rm = TRUE) #population sd
cat('The population mean penguin weight is:',mu,'with population standard deviation',sigma)
#Take random sample of size greater than 30 with replacement.
n <- 100
sample_weights <- sample(df$weight, n, replace = TRUE) # Added replace = TRUE for sampling with replacement
#CONFIDENCE INTERVAL
#Construct 95% CI for weights based on sample
xbar <- mean(sample_weights, na.rm = TRUE) #sample mean
s <- sd(sample_weights, na.rm = TRUE) #sample sd
zc <- qnorm(0.975) # Corrected to only calculate once for the upper tail
se <- s/sqrt(n) # Changed to use sample standard deviation 's' instead of population 'sigma'
E <- zc*se
lower_bound_CI <- xbar - E
upper_bound_CI <- xbar + E
cat('95% confidence interval for population mean: (',lower_bound_CI,',',upper_bound_CI,')')
#TWO TAILED HYPOTHESIS TEST
#Step 1: HO: mu = mean(weights),  HA: mu != mean(weights)
claim <- "H0"
#Step 2:
alpha <- 0.05
zc <- qnorm(1 - alpha/2) # Corrected to find the critical value for both tails
#Step 3:
xbar <- mean(sample_weights, na.rm = TRUE)
mu_xbar = mu
se <- sigma/sqrt(n)
z = (xbar-mu_xbar)/se
if (!is.na(z)) { # Added check to ensure z is not NA before proceeding
if (z > 0) {
p <- (1- pnorm(z,0,1))*2
} else {
p <- pnorm(z,0,1)*2
}
z
p
#Step 4:
if (p > 0.05){
cat('Since p > 0.05: do not reject H0.')
rejectH0 <- FALSE
} else {
cat('Since p < 0.05: reject H0.')
rejectH0 <- TRUE
}
#Step 5
if (claim == "H0" & rejectH0 == TRUE){
cat('There is enough evidence to reject the claim.')
} else if (claim == "H0" & rejectH0 == FALSE) {
cat('There is not enough evidence to reject the claim.')
} else if (claim == "HA" & rejectH0 == TRUE) {
cat('There is enough evidence to support the claim.')
} else {
cat('There is not enough evidence to support the claim.')
}
}
#Confirm CI and hypothesis test with z.test function
z.test(sample_weights, mu = mu, sigma.x = sigma, conf.level = 0.95)
# Calculate sample mean and standard deviation
xbar <- mean(df$weight, na.rm = TRUE) # Sample mean
s <- sd(df$weight, na.rm = TRUE) # Sample standard deviation
n <- nrow(df) # Sample size
cat('The sample mean penguin weight is:',xbar,'with sample standard deviation',s)
#CONFIDENCE INTERVAL
#Construct 95% CI for weights based on your sample
tc <- qt(0.975,n-1)
se <- s/sqrt(n)
E <- tc*se
lower_bound_CI <- xbar - E
upper_bound_CI <- xbar + E
cat('95% confidence interval for population mean: (',lower_bound_CI,',',upper_bound_CI,')')
# Bootstrap estimation for population mean weight in grams
# 1. Ensure the 'weight' column is numeric, in case it's not
df$weight <- as.numeric(as.character(df$weight))
# 2. Bootstrap Function
bootstrap_mean <- function(df, n_bootstrap) {
bootstrap_means <- numeric(n_bootstrap)
for (i in 1:n_bootstrap) {
sample_indices <- sample(1:length(df), replace = TRUE)
sample_data <- df[sample_indices]
bootstrap_means[i] <- mean(sample_data, na.rm = TRUE)
}
return(bootstrap_means)
}
# 3. Generate Bootstrap Distribution
# Assuming we want to perform 1000 bootstrap resamples
n_bootstrap <- 15000
bootstrap_distribution <- bootstrap_mean(df$weight, n_bootstrap)
# 4. Estimate Mean and Confidence Interval
estimated_mean <- mean(bootstrap_distribution)
conf_interval <- quantile(bootstrap_distribution, probs = c(0.025, 0.975))
# Print the results
print(paste("Estimated Mean:", estimated_mean))
print(paste("95% Confidence Interval:", conf_interval[1], "-", conf_interval[2]))
# TWO TAILED HYPOTHESIS TEST
# Define hypotheses
# Null hypothesis (HO): mu = 4202
# Alternative hypothesis (HA): mu != 4202
# Step 1: Significance level
alpha <- 0.05
# Step 2: Calculate test statistic
mu_0 <- 4202 # Hypothesized population mean
se <- s / sqrt(n) # Standard error
t <- (xbar - mu_0) / se # Test statistic
# Step 3: Calculate p-value for two-tailed test
p <- 2 * (1 - pt(abs(t), n - 1))
cat('Test statistic:', t, '\n')
cat('P-value:', p, '\n')
# Step 4: Decision rule
if (p > alpha) {
cat('Since p > alpha: do not reject H0.\n')
rejectH0 <- FALSE
} else {
cat('Since p <= alpha: reject H0.\n')
rejectH0 <- TRUE
}
# Step 5: Conclusion
if (!rejectH0) {
cat('There is not enough evidence to reject the null hypothesis (HO).\n')
} else {
cat('There is enough evidence to reject the null hypothesis (HO).\n')
}
#Confirm CI and hypothesis test with z.test function
result <- t.test(df$weight, mu = 4202) #for sample data set
#result <- t.test(sample_heights, mu = 78) #for populations data set
result
#z.test(sample_heights, mu = mu, sigma.x = sigma, conf.level = 0.95)
library(tidyverse)
library(multcomp)
library(car)  # for Levene's test
library(MASS) # for Box-Cox transformation
# Remove rows with missing values in the variables of interest
penguins_complete <- na.omit(df[, c("species", "weight")])
# Perform Levene's test for equal variances
leveneTest(weight ~ species, data = penguins_complete)
### Levene's test shows that the variances are definitely
### not equal, so a standard ANOVA test should not be used.
###### Alternative 1: Welch's ANOVA
anova_results <- oneway.test(weight ~ species, data = penguins_complete, var.equal = FALSE)
# Output the results of Welch's ANOVA
anova_results
###### Alternative 2: Box-Cox Transformation
# Apply the Box-Cox transformation
bc <- boxcox(weight ~ species, data = penguins_complete)
# Find the lambda that maximizes the log-likelihood
best_lambda <- bc$x[which.max(bc$y)]
# Transform the data using the best lambda
penguins_complete$weight_transformed <- (penguins_complete$weight^best_lambda - 1) / best_lambda
# Diagnostic plot to assess normality of transformed data
hist(penguins_complete$weight_transformed, main = "Histogram of Transformed Body Mass", xlab = "Transformed Body Mass")
qqnorm(penguins_complete$weight_transformed)
qqline(penguins_complete$weight_transformed)
# The plot should be a straight line. It's not.
###### Alternative 3: Log Transformation of weight
# Apply log transformation
penguins_complete$weight_log <- log(penguins_complete$weight)
# We can now proceed with the analysis using the transformed data
# The following example performs a simple linear model (ANOVA) on the transformed data
anova_log <- aov(weight_log ~ species, data = penguins_complete)
# Output the summary of the ANOVA
summary(anova_log)
##### All three alternatives reach the same conclusion:
##### The null hypothesis that the population means are
##### equal should be rejected.
m <- aov(weight ~ species, df)
summary(m)
# Perform Tukey's HSD test
tukey_results <- TukeyHSD(m, conf.level = 0.95)
# Print the results of Tukey's HSD test
print(tukey_results)
library(binom)
library(tidyverse)
library(dplyr)
library(stats)
# Assuming the proportion of Adelie penguins in the dataset
# is our population proportion
p_population <- mean(df$species == "Adelie")
# Take a random sample of size 100 from the dataset
set.seed(666) # For reproducibility
sample_data <- df[sample(nrow(df), 100), ]
# Calculate the sample proportion of Adelie penguins
p_sample <- mean(sample_data$species == "Adelie")
# Construct a 95% confidence interval for the population proportion
conf_interval <- binom.confint(x = sum(sample_data$species == "Adelie"),
n = 100,
conf.level = 0.95,
methods = "exact")
# Extract lower and upper confidence limits
lower_bound <- conf_interval$lower
upper_bound <- conf_interval$upper
# Conduct a z hypothesis test
z_test <- prop.test(x = sum(sample_data$species == "Adelie"),
n = 100,
p = p_population,
alternative = "two.sided",
conf.level = 0.95)
# Print the results
print(conf_interval)
print(z_test)
cat('The population proportion of Adelie penguins is ',p_population)
cat('The 95% confidence interval for the population proportion of Adelie penguins is (',
lower_bound, ', ', upper_bound, ')', sep = '')
# TWO TAILED HYPOTHESIS TEST
# Step 1: Null and Alternative Hypotheses
# H0: p = 0.44, HA: p != 0.44
population_p <- 0.44
claim <- "H0"
# Step 2: Define alpha level and critical z value
alpha <- 0.05
zc <- qnorm(1 - alpha/2) # critical value for two-tailed test
# Step 3: Calculate sample proportion and z statistic
phat <- mean(df$species == "Adelie") # sample proportion of Adelie penguins
n <- length(df$species) # sample size
se <- sqrt(population_p * (1 - population_p) / n) # standard error
z <- (phat - population_p) / se # z statistic
# Calculate p-value
p_value <- 2 * (1 - pnorm(abs(z))) # two-tailed p-value
# Print z statistic and p-value
cat("Z-statistic:", z, "\nP-value:", p_value)
# Step 4: Decision rule
rejectH0 <- FALSE
if (p_value < alpha) {
cat('Since p < alpha: reject H0.\n')
rejectH0 <- TRUE
} else {
cat('Since p >= alpha: do not reject H0.\n')
}
# Step 5: Conclusion based on the claim
if (claim == "H0" && rejectH0 == TRUE) {
cat('There is enough evidence to reject the claim.\n')
} else if (claim == "H0" && rejectH0 == FALSE) {
cat('There is not enough evidence to reject the claim.\n')
} else if (claim == "HA" && rejectH0 == TRUE) {
cat('There is enough evidence to support the claim.\n')
} else {
cat('There is not enough evidence to support the claim.\n')
}
# Confirm CI with binom.confint
num_adelie <- sum(df$species == "Adelie")
# Asymptotic (Wald) confidence interval
conf_int <- binom.confint(x = num_adelie, n = n, method = "asymptotic", conf.level = 1 - alpha)
# Print the confidence interval
cat('The 95% confidence interval for the population proportion of Adelie penguins is (',
conf_int$lower, ', ', conf_int$upper, ')\n', sep = '')
# One-sample test for the proportion of Adélie penguins
# Testing against the null hypothesis that the proportion is 0.44
num_adelie <- sum(df$species == "Adelie")
n <- length(df$species) # Total number of observations in the sample
# Perform the test
one_prop_test <- prop.test(x = num_adelie, n = n, p = population_p, alternative = "two.sided", conf.level = 0.95)
# Print the results
print(one_prop_test)
###### Chi square goodness of fit test
### to test the distribution of species in the penguins dataset
# Set seed for reproducibility
set.seed(666)
# Observe actual frequencies for each species
actual_frequencies <- table(df$species)
# Assuming we expect the species to be evenly distributed
expected_frequencies <- rep(1/length(unique(df$species)), length(unique(df$species)))
# Perform chi-square goodness of fit test
chi_square_test <- chisq.test(x = actual_frequencies, p = expected_frequencies)
# Output the result of the chi-square test
chi_square_test
# Chi-squared test for given probabilities
#
# data:  actual_frequencies
# X-squared = 31.907, df = 2, p-value = 1.179e-07
##### Chi-square test for independence
### between `species` and `island`
# Test the independence between 'species' and 'island' in the penguins dataset
# Create a contingency table of the two categorical variables
contingency_table <- table(df$species, df$island)
# Perform chi-square test of independence
chi_square_independence_test <- chisq.test(contingency_table)
# Output the result of the chi-square test of independence
chi_square_independence_test
# Pearson's Chi-squared test
#
# data:  contingency_table
# X-squared = 299.55, df = 4, p-value < 2.2e-16
# Remove NA values
df %>% drop_na()
# Fit the multinomial logistic regression model
multinom_model <- multinom(species ~ culmen_length_mm + culmen_depth_mm + flipper_length_mm + weight, data = df)
# Print the model summary
summary(multinom_model)
# Call:
#   multinom(formula = species ~ culmen_length_mm + culmen_depth_mm +
#              flipper_length_mm + weight, data = df)
#
# Coefficients:
#   (Intercept) culmen_length_mm culmen_depth_mm flipper_length_mm
# Chinstrap  -34.806703         56.16797       -80.32042         -2.546743
# Gentoo      -4.446499         38.97704       -87.02802         -1.281737
# weight
# Chinstrap -0.12676681
# Gentoo     0.02163804
#
# Std. Errors:
#   (Intercept) culmen_length_mm culmen_depth_mm flipper_length_mm
# Chinstrap 3.330608e+00     53.187991536    3.848396e+01      11.539675662
# Gentoo    3.951616e-05      0.001697574    7.008909e-04       0.007754504
# weight
# Chinstrap 0.2522008
# Gentoo    0.1856241
#
# Residual Deviance: 0.001556389
# AIC: 20.00156
# Predict the training data
df$pred <- predict(multinom_model, newdata = df, type = "class")
# Calculate accuracy
confusionMatrix(df$pred, df$species)
# Confusion Matrix and Statistics
#
# Reference
# Prediction  Adelie Chinstrap Gentoo
# Adelie       151         0      0
# Chinstrap      0        68      0
# Gentoo         0         0    123
#
# Overall Statistics
#
# Accuracy : 1
# 95% CI : (0.9893, 1)
# No Information Rate : 0.4415
# P-Value [Acc > NIR] : < 2.2e-16
#
# Kappa : 1
#
# Mcnemar's Test P-Value : NA
#
# Statistics by Class:
#
#                      Class: Adelie Class: Chinstrap Class: Gentoo
# Sensitivity                 1.0000           1.0000        1.0000
# Specificity                 1.0000           1.0000        1.0000
# Pos Pred Value              1.0000           1.0000        1.0000
# Neg Pred Value              1.0000           1.0000        1.0000
# Prevalence                  0.4415           0.1988        0.3596
# Detection Rate              0.4415           0.1988        0.3596
# Detection Prevalence        0.4415           0.1988        0.3596
# Balanced Accuracy           1.0000           1.0000        1.0000
# Write new csv and rds files
