---
title: "Exploring Biodiversity Metrics: A Comprehensive Statistical Analysis of the Palmer Penguins Dataset"
output: html_notebook
---

author: Andrex Ibiza, MBA organization: Johnson & Wales University date: 2024-04-14

**Abstract:**

This paper was written as a series of weekly submissions over 15 weeks as part of the DATA5150 - Statistical Analysis course at Johnson & Wales University. This research aims to leverage the Palmer Penguins dataset to conduct a comprehensive statistical analysis that elucidates patterns of biodiversity and conservation-relevant metrics within the penguin populations of Antarctica's Palmer Archipelago. Utilizing a range of statistical techniques, this study seeks to uncover the interdependencies among various biological markers such as species type, bill dimensions, flipper length, and body mass. By providing deeper insights into the ecological dynamics and adaptive traits of these species, this analysis will contribute to ongoing conservation efforts and the broader understanding of Antarctic marine ecosystems.

# 1. Introduction

The Palmer Penguins dataset has emerged as a pivotal tool for ecologists and data scientists aiming to understand the complexities of biodiversity within the Antarctic region. Introduced by Horst AM, Hill AP, and Gorman KB, the dataset encompasses measurements from three penguin species—Adélie, Chinstrap, and Gentoo—found on three islands in the Palmer Archipelago, Antarctica. This rich dataset includes variables such as species, island, bill length, bill depth, flipper length, body mass, and sex, offering a unique opportunity to explore the morphological diversification influenced by ecological and evolutionary pressures.

## Significance of the Study

The conservation of Antarctic marine species is increasingly pressing in the face of climatic and environmental shifts. Understanding species-specific responses to environmental changes through detailed morphological and demographic data is crucial for predicting future impacts on these populations and formulating effective conservation strategies. The Palmer Penguins dataset not only facilitates a granular analysis of interspecies characteristics but also serves as a proxy for gauging the health of the broader marine ecosystem in the Antarctic region.

## Objectives of the Study

The primary objectives of this study are to:

-   Conduct a detailed exploratory data analysis to identify the primary characteristics and distributions within the dataset.
-   Apply advanced statistical methods to assess correlations and develop predictive models concerning penguin species' physical characteristics and their ecological implications.
-   Explore the dataset’s utility in addressing ecological questions pertinent to conservation efforts, such as the impact of environmental variables on species distribution.

## Research Question

This study addresses the following research questions:

1.  What are the key morphological differences among the three penguin species, and how do these relate to ecological roles and behaviors?

## Structure of the Paper

The remainder of the paper is structured as follows:

By utilizing a comprehensive statistical framework, this study aims to provide insightful analyses that support conservation strategies and enhance our understanding of the ecological dynamics influencing Antarctic penguin populations.

# 2. Data Loading, Cleaning, and Exploratory Data Analysis

The first steps in the project are to load the data into a dataframe and begin a cursory inspection of its overall features.

The `penguins_raw` dataset from the `palmerpenguins` package contains data on 344 penguin observations with 17 variables. The original variables in the dataset were “studyName”, “Sample Number”, “Species”, “Region”, “Island”, “Stage”, “Individual ID”, “Clutch Completion”, “Date Egg”, “Culmen Length (mm)”, “Culmen Depth (mm)”, “Flipper Length (mm)”, “Body Mass (g)”, “Sex”, “Delta 15 N (o/oo)”, “Delta 13 C (o/oo)” and “Comments”.

I made the following changes to the dataset: - `studyName`: changed to `study_name`\
- `Sample Number`: changed to `sample_number` and converted to integer\
- `Species`: changed to `species`, converted to factor, names shortened\
- `Region`: changed to `region` and converted to factor\
- `Island`: changed to `island` and converted to factor\
- `Stage`: changed to `stage` and converted to factor\
- `Individual ID`: changed to `individual_id`\
- `Clutch Completion`: changed to `clutch_completion` and converted to factor\
- `Date Egg`: changed to `date_egg`\
- `Culmen Length (mm)`: changed to `culmen_length_mm`\
- `Culmen Depth (mm)`: changed to `culmen_depth_mm`\
- `Flipper Length (mm)`: changed to `flipper_length_mm`\
- `Body Mass (g)`: changed to `weight`\
- `Sex`: changed to `sex` and converted to factor\
- `Delta 15 N (o/oo)`: changed to `delta_15_n`\
- `Delta 13 C (o/oo)`: changed to `delta_13_c`\
- `Comments`: changed to `comments`\

The cleansed dataset contains the following variables: - `study_name` (character) – Sampling expedition from which data were collected, generated, etc.\
- `sample_number` (integer) – an integer denoting the continuous numbering sequence for each sample\
- `species` (factor) – 3 levels: “Adelie", "Chinstrap", and "Gentoo"\
- `region` (factor) – only 1 level, “Anvers”, denoting the region of Palmer LTER sampling grid\
- `island` (factor) – 3 levels – “Biscoe”, “Dream”, and “Torgersen” denoting the island near Palmer Station where samples were collected\
- `stage` (factor) – 1 level – “Adult, 1 Egg Stage” – denotes reproductive stage at sampling\
- `individual_id` (character) – alphanumeric unique identifier for each individual penguin represented in the data\
- `clutch_completion` (factor) – 2 factors – “Yes” or “No” – denoting whether the study nest was observed with a full clutch, i.e. 2 eggs\
- `date_egg` (date) denotes the date study nest observed with one egg (sampled)\
- `culmen_length_mm` (numeric) – a number in millimeters denoting the length of the dorsal ridge of a bird’s bill\
- `culmen_depth_mm` (numeric) – a number in millimeters denoting the depth of the dorsal ridge of a bird’s bill\
- `flipper_length_mm` (numeric) – a number in millimeters denoting the length of the penguin’s flipper\
- `weight` (numeric) – a number denoting the body mass of the penguin in grams\
- `sex` (factor) – denotes penguin’s sex\
- `delta_15_n` (numeric) - a number denoting the measure of the ratio of stable isotopes 15N:14N\
- `delta_13_c` (numeric) - a number denoting the measure of the ratio of stable isotopes 13C:12C\
- `comments` (character) – text providing additional relevant information for the data\

The dimensions of the cleansed dataset are 344 x 17.

```{r}
# Install and load packages
install.packages("palmerpenguins")
install.packages("naniar")
install.packages("gt")
install.packages("BSDA")
library(palmerpenguins) #data source
library(tidyverse) #dplyr, ggplot2, tidyr
library(naniar) #working with missing data
library(gt) #for nice tables
library(BSDA)

# Set df
df <- penguins_raw
```

```{r}
# Print the first 10 rows of the data set
head(df, 10)
```

```{r}
# Print the last 10 rows of the data set
tail(df, 10)
```

```{r}
# Find the dimensions of the data set
dim(df)
```

```{r}
# Determine the names of the variables in the data set
colnames(df)
```

```{r}
# Determine the structure of the variables in the data set
str(df)
```

## Data Cleaning

```{r}
# Reformat all column names
colnames(df)[1] <- "study_name"
colnames(df)[2] <- "sample_number"
colnames(df)[3] <- "species"
colnames(df)[4] <- "region"
colnames(df)[5] <- "island"
colnames(df)[6] <- "stage"
colnames(df)[7] <- "individual_id"
colnames(df)[8] <- "clutch_completion"
colnames(df)[9] <- "date_egg"
colnames(df)[10] <- "culmen_length_mm"
colnames(df)[11] <- "culmen_depth_mm"
colnames(df)[12] <- "flipper_length_mm"
colnames(df)[13] <- "weight"
colnames(df)[14] <- "sex"
colnames(df)[15] <- "delta_15_n"
colnames(df)[16] <- "delta_13_c"
colnames(df)[17] <- "comments"

# Recheck column names
colnames(df) # column re-naming successful!
```

```{r}
# Load packages
library(palmerpenguins) # data source
library(dplyr)
library(tidyverse)
library(naniar) # working with missing data
library(tidyr) # manipulating character strings

# Determine column names
colnames(df)

# Check for missing data values
vis_miss(df)

# Check for missing data
miss_var_summary(df)

## There is missing data in: 
## "comments" (84.3% miss.), 
## "delta_15_n" (4.07%), 
## "delta_13_c" (3.78%), 
##"sex" (3.20%), 
##"culmen_length_mm" (0.581%),
## "culmen_depth_mm" (0.581%),
## flipper_length_mm" (0.581%), 
## "weight" (0.581%)
```

```{r}
str(df)
```

### Verify missing values listed as `NA`

```{r}
# View columns listed above to ensure missing data is listed as NA
df$comments # all are NA
df$delta_15_n # all are NA
df$delta_13_c # all are NA
df$sex # all are NA
df$culmen_length_mm # all are NA
df$culmen_depth_mm # all are NA
df$flipper_length_mm # all are NA
df$weight # all are NA
```

```{r}
# Check that the sum of the number of good data values and the number of missing 
# data values is equal to the total number of data values for the columns
sum(is.na(df$comments)) + sum(!is.na(df$comments)) - nrow(df) # 0
sum(is.na(df$delta_15_n)) + sum(!is.na(df$delta_15_n)) - nrow(df) # 0
sum(is.na(df$delta_13_c)) + sum(!is.na(df$delta_13_c)) - nrow(df) # 0
sum(is.na(df$sex)) + sum(!is.na(df$sex)) - nrow(df) # 0
sum(is.na(df$culmen_length_mm)) + sum(!is.na(df$culmen_length_mm)) - nrow(df) # 0
sum(is.na(df$culmen_depth_mm)) + sum(!is.na(df$culmen_depth_mm)) - nrow(df) # 0
sum(is.na(df$flipper_length_mm)) + sum(!is.na(df$flipper_length_mm)) - nrow(df) # 0
sum(is.na(df$weight)) + sum(!is.na(df$weight)) - nrow(df) # 0
```

### Clean data types

```{r}
# Convert sample_number to an integer
df$sample_number <- as.integer(df$sample_number)
class(df$sample_number)

# Convert species, region, island, stage, clutch_completion, and sex to factors
df$species <- as.factor(df$species)
class(df$species)

df$region <- as.factor(df$region)
class(df$region)

df$island <- as.factor(df$island)
class(df$island)

df$stage <- as.factor(df$stage)
class(df$stage)

df$clutch_completion <- as.factor(df$clutch_completion)
class(df$clutch_completion)

df$sex <- as.factor(df$sex)
class(df$sex)

# Recheck the structure of the variables in the data set
str(df)
```

### Clean and verify factor levels

```{r}
# Rename species levels for simplicity
levels(df$species) <- c("Adelie", "Chinstrap", "Gentoo")

# Inspect factor levels for preparation of summary report
levels(df$species)
levels(df$region)
levels(df$island)
levels(df$stage)
levels(df$clutch_completion)
levels(df$sex)
```

# 3. Exploratory Data Visualizations

## Categorical Data Visualizations

```{r}
# Scatter plot culmen_length_mm x culmen_depth_mm color by species
posn_j <- position_jitter(0.1,
                          seed = 666)
ggplot(df,
       aes(x = culmen_length_mm,
           y = culmen_depth_mm,
           color = species)) +
  geom_point(position = posn_j, alpha = 0.7, size = 5) +
  labs(title="Culmen Length (mm) vs. Culmen Length (mm) by Species",
       x = "Culmen Length (mm)",
       y = "Culmen Depth (mm)") +
  scale_color_discrete("Species") +
  theme_classic()
```

### Scatter plot interpretation

The scatter plot examines the relationship between culmen length and depth, showing clear differentiation among species. Adelie penguins tend to have shorter, deeper culmens, while Gentoo penguins exhibit longer, shallower beaks, possibly reflecting diverse feeding strategies.

```{r}
# Boxplot of weight by species
ggplot(df, 
       aes(x=species, 
           y=weight, 
           fill=species)) +
  geom_boxplot() +
  labs(title = "Weight by Species", 
       x = "Species", 
       y = "Weight") +
  theme_classic()
```

### Box plot interpretation

The box plot (Figure 2) of body mass by species indicates a higher median body mass for Gentoo penguins, with their box and whiskers suggesting greater variation in mass. Notable outliers in the Chinstrap data suggest individual variations or data recording anomalies.

```{r}
# Stacked density plots of weight colored by species
ggplot(df, aes(x=weight, fill=species)) +
  geom_density(alpha = 0.5) +
  labs(title = "Stacked Density Plots of Weight by Species", x = "Body Mass (g)", y = "Density") +
  theme_minimal() +
  scale_y_continuous(labels = scales::comma)
```

```{r}
# Stacked density plots of culmen_length_mm colored by species
ggplot(df, aes(x=culmen_length_mm, fill=species)) +
  geom_density(alpha = 0.5) +
  labs(title = "Stacked Density Plots of Culmen Length (mm) by Species", x = "Culmen Length (mm)", y = "Density") +
  theme_classic() +
  scale_y_continuous(labels = scales::comma)
```

```{r}
# Stacked density plots of culmen_depth_mm colored by species
ggplot(df, aes(x=culmen_depth_mm, fill=species)) +
  geom_density(alpha = 0.5) +
  labs(title = "Stacked Density Plots of Culmen Depth (mm) by Species", x = "Culmen Depth (mm)", y = "Density") +
  theme_classic() +
  scale_y_continuous(labels = scales::comma)
```

### Stacked Density Plot Interpretation

Stacked density plots for weight, culmen length (Figure 4), and culmen depth (Figure 5) reveal overlapping distributions, indicating a shared environmental influence on body mass, while the distinct peaks for culmen length underscore species-specific evolutionary traits.

```{r}
# Stacked bar plot for species by island
ggplot(df, aes(x = island, fill = species)) +
  geom_bar(position = "stack") +
  labs(title = "Segmented Bar Plot of Species by Island", x = "Island", y = "Count") +
  theme_classic()
```

### Segmented bar plot interpretation

-   Gentoo penguins appear to be predominantly on Biscoe Island.
-   Adelie penguins are found on all three islands, but their population is dominant on Torgersen Island.
-   Chinstrap penguins are found in a significant number on Dream Island, but not on the others.

### Categorical Visualizations - Conclusion

In conclusion, the visual data analyzed paints a compelling picture of the unique evolutionary trajectories undertaken by the Adelie, Chinstrap, and Gentoo penguins. The distinctions in culmen dimensions and body mass among these species not only reflect their specialized ecological niches but also their adaptability to the dynamic Antarctic environment. These biometric variations may influence foraging strategies, reproductive success, and interspecies competition. Looking forward, these insights prompt further inquiry into the resilience of these species amidst climatic shifts, guiding conservation efforts and enhancing our understanding of Antarctic biodiversity's fragility. Moreover, the biometric data studied could act as a sentinel for broader environmental changes, anchoring future ecological and environmental research. Through such studies, we can better predict and mitigate the impacts of global environmental changes on these emblematic avian species.

## Numeric Data Visualization

First, we examined the distributions of all of the numeric variables in the dataset: `body_mass_g`, `culmen_depth_mm`, `culmen_length_mm`, `delta_13_c`, `delta_15_n`, and `flipper_length_m`. Applying the Empirical Rule to any of these variables requires that the variable be approximately symmetrical in its distribution. None of these variables appear to meet this criterion.

```{r}
# Subsetting penguins_clean by only columns with numerical data
penguins_numeric <- df[sapply(df, is.numeric)]
str(penguins_numeric)
```

```{r}
# Reshape the data
penguins_long <- penguins_numeric %>%
  pivot_longer(cols = -sample_number, names_to = "variable", values_to = "value")

# Create faceted histograms for each variable in penguins_numeric
ggplot(penguins_long, aes(x = value)) +
  geom_histogram(bins = 30, fill = 'red', color = 'black') +
  facet_wrap(~ variable, scales = 'free_x') +
  theme_minimal() +
  labs(title = 'Faceted Histograms for Each Numeric Variable in Penguins Dataset',
       x = 'Value',
       y = 'Frequency')
```

### Weight Histogram

The histogram for body mass appears to be roughly unimodal and slightly right-skewed, indicating that most penguins have a body mass around the center of the distribution with fewer penguins having higher body masses. The empirical rule, which states that approximately 68%, 95%, and 99.7% of the data fall within one, two, and three standard deviations from the mean, respectively, assumes a normal distribution. Given the skewness observed, the empirical rule may not strictly apply here.

### Culmen Depth (mm) Histogram

The culmen depth shows a bimodal distribution with two distinct peaks. This suggests there are possibly two different subgroups within the penguin population with different average culmen depths. Since the distribution is not normal and has multiple peaks, the empirical rule would not be appropriate for this variable.

### Culmen Length (mm) Histogram

Similar to culmen depth, the culmen length is also bimodal, which suggests the presence of two subgroups with different culmen lengths. The bimodal nature again indicates that the empirical rule would not be appropriate here.

### Delta 13 C (‰) Histogram

The distribution of delta 13 C appears multimodal with multiple peaks, which might indicate the presence of different dietary or environmental subgroups within the penguins. Multimodal distributions are not suitable for the application of the empirical rule, as it does not follow a normal distribution pattern.

### Delta 15 N (‰) Histogram

This histogram is also multimodal, similar to delta 13 C, which suggests complexity in the data that might be related to different feeding habits or habitats. The empirical rule cannot be applied due to the lack of a normal distribution.

### Flipper Length (mm) Histogram

The histogram for flipper length appears unimodal with a slight skew. There is one prominent peak, and the data tails off as flipper length increases. While there is a single peak, the skewness might still pose an issue for the empirical rule, although this variable might be the best candidate for its application compared to the others if one considers applying some skewness correction.

In summary, the empirical rule is most useful for data that are symmetrically distributed and bell-shaped (normal distribution). In these histograms, many of the variables show signs of bimodal or multimodal distributions, which suggests that different subpopulations exist within the data. The empirical rule does not apply well to such distributions.

## Applying Chebyshev's Rule to Numeric Variables

Chebyshev's Rule states that for any dataset and for any constant $k > 1$, at least $(1 - 1/k^2)$ of the data values must lie within $k$ standard deviations of the mean. We can apply this rule to the numeric variables in the `penguins_numeric` dataset to determine the proportion of data that lies within a certain number of standard deviations from the mean.

```{r}
# Function to apply Chebyshev's Rule
apply_chebyshev <- function(data, k) {
  # Exclude the "sample_number" column
  data <- data[, !colnames(data) %in% "sample_number"]

  # Calculate mean and standard deviation for each numeric column
  stats <- sapply(data, function(column) {
    c(mean = mean(column, na.rm = TRUE), sd = sd(column, na.rm = TRUE))
  })

  # Apply Chebyshev's Rule to calculate the minimum proportion within k standard deviations
  chebyshev_min_proportion <- 1 - 1 / (k^2)

  # Calculate the range within k standard deviations
  ranges <- t(stats) * c(1, k) + cbind(stats['mean',], -stats['sd',])

  # Count the number of values within the range
  within_range <- sapply(data, function(column, range) {
    sum(column >= range[1] & column <= range[2], na.rm = TRUE)
  }, range = ranges)

  # Calculate the proportion of values within the range
  proportions <- within_range / sapply(data, function(column) sum(!is.na(column)))

  # Return a list with the calculated values
  list(
    k = k,
    chebyshev_min_proportion = chebyshev_min_proportion,
    ranges = ranges,
    proportions = proportions
  )
}

# Exclude the "sample_number" column from penguins_numeric
penguins_numeric <- penguins_numeric[, !colnames(penguins_numeric) %in% "sample_number"]

# Calculate mean and standard deviation for each numeric column
chebyshev <- sapply(penguins_numeric, function(column) {
  c(mean = mean(column, na.rm = TRUE), sd = sd(column, na.rm = TRUE))
})

# Print the results
chebyshev
```

To explain the results of applying Chebyshev's Rule to the `penguins_numeric` dataset, we need to understand what Chebyshev's Rule is and what it tells us about the data.

Chebyshev's Rule, also known as Chebyshev's Inequality, is a statistical rule that provides a lower bound on the probability that a random variable lies within a certain number of standard deviations from the mean. Specifically, for any real number $k > 1$, at least $(1 - \frac{1}{k^2})$ of the values lie within $k$ standard deviations of the mean. This rule applies to any probability distribution, regardless of its shape.

In the context of the `penguins_numeric` dataset, the function `apply_chebyshev` was used to calculate the minimum proportion of values that should fall within $k$ standard deviations from the mean for each numeric column. The function was called with $k = 2$, which means we are looking at the range within two standard deviations from the mean.

According to Chebyshev's Rule, with $k = 2$, at least $(1 - \frac{1}{2^2}) = 0.75$ or 75% of the data should fall within this range. The function then calculates the actual ranges for each numeric column and determines the proportion of values that fall within these ranges.

The results will include:

-   The value of $k$ used in the calculation.
-   The minimum proportion of values that should fall within $k$ standard deviations from the mean according to Chebyshev's Rule.
-   The actual ranges calculated for each numeric column.
-   The proportion of values that fall within the calculated ranges for each column.

These results can be used to understand the spread and distribution of the data in the `penguins_numeric` dataset. If the actual proportions are significantly higher than the minimum proportions given by Chebyshev's Rule, it indicates that the data is less spread out (i.e., more values are close to the mean). If the proportions are close to the minimum, it suggests that the data is more spread out.

### Outlier Detection Using Z-scores

To determine if there are any outliers in the dataset, we can calculate the z-scores for each numerical variable in the `penguins_numeric` dataset. A z-score indicates how many standard deviations an element is from the mean. A common threshold for identifying outliers is a z-score of greater than 3 or less than -3.

Let's calculate the z-scores for the numerical variables and identify any outliers.

```{r}
# Calculate z-scores for the numerical variables in the penguins_numeric dataset
z_scores <- as.data.frame(scale(penguins_numeric[,-1])) # Exclude sample_number
names(z_scores) <- names(penguins_numeric[,-1]) # Assign correct column names

# Check for outliers (z-score > 3 or z-score < -3)
outliers <- apply(z_scores, 2, function(x) sum(x > 3 | x < -3))

# Output the number of outliers for each variable
outliers
```

None of these variables had any outliers +/- 3 standard deviations from the mean.

## Coefficient of Variation Comparison

The coefficient of variation (CV) is a statistical measure of the dispersion of data points in a data series around the mean. It is calculated as the ratio of the standard deviation to the mean, and it is often expressed as a percentage. The CV is useful because it allows for comparison of the variability of different datasets with different units or means.

We will calculate the CV for two or more numerical variables from the `penguins_numeric` dataset and compare their variability.

```{r}
# Calculate the coefficient of variation for selected numerical variables
coefficient_of_variation <- function(x) {
  (sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE)) * 100
}

# Select variables to compare
selected_variables <- c('culmen_length_mm', 'flipper_length_mm', 'weight')

# Calculate CV for each selected variable
cv_values <- sapply(penguins_numeric[selected_variables], coefficient_of_variation)

# Output the CV values
cv_values
```

### Explanation of Coefficients of Variation

The coefficients of variation (CV) for the variables `culmen_length_mm`, `flipper_length_mm`, and `body_mass_g` from the `penguins_numeric` dataset are 12.43%, 6.999%, and 19.086% respectively. Here's what these values indicate about the variability of each measurement:\

-   **Culmen Length (mm)**: With a CV of 12.43%, this suggests that the culmen length measurements have a moderate level of variability relative to the mean culmen length. This means that while there is some variation in culmen length among the penguins, it is not excessively high.\

-   **Flipper Length (mm)**: The CV of 6.999% for flipper length indicates that this variable has relatively low variability compared to its mean. This implies that the flipper lengths of the penguins are quite consistent, with less variation from the average flipper length.\

-   **Body Mass (g)**: The body mass has a CV of 19.086%, which is higher than the other two variables. This higher CV suggests that there is a greater level of variability in the body mass of the penguins. In other words, the weights of the penguins are more spread out from the average weight, indicating a wider range of body mass values within the dataset.\

In summary, the CV provides a standardized measure of variability that is independent of the unit of measurement. By comparing the CVs, we can conclude that the body mass of the penguins is the most variable trait among the three, while flipper length is the least variable.

### Applying Chebyshev's Inequality to `culmen_length_mm`

Chebyshev's inequality states that for any dataset and for any constant $k > 1$, the proportion of values that lie within $k$ standard deviations of the mean is at least $1 - 1/k^2$. We can apply this rule to the `culmen_length_mm` variable to find the range within which a certain proportion of data points are expected to lie.

```{r}
# Calculate the mean and standard deviation for culmen_length_mm
mean_culmen_length <- mean(penguins_numeric$culmen_length_mm, na.rm = TRUE)
sd_culmen_length <- sd(penguins_numeric$culmen_length_mm, na.rm = TRUE)

# Define k for Chebyshev's inequality (e.g., k = 2 for 75%)
k <- 2

# Calculate the range using Chebyshev's inequality
minus_k_sd <- round(mean_culmen_length - k*sd_culmen_length, 1)
plus_k_sd <- round(mean_culmen_length + k*sd_culmen_length, 1)
percent <- (1 - 1/k^2)*100

cat("By Chebyshev's Rule, at least", percent, "% of the penguins' culmen lengths are between", minus_k_sd, "and", plus_k_sd, "mm.")
```

```{r}
library(ggplot2)
library(dplyr)
library(gt)

weights_freq_distribution <- data.frame(table(df$weight))
weights <- as.numeric(as.character(weights_freq_distribution$Var1))
probabilities <- round(weights_freq_distribution$Freq/sum(weights_freq_distribution$Freq), 4)
sum(probabilities) #should sum to 1 

# Construct a discrete probability distribution of weights
weights_discrete_prob_distribution <- data.frame(weights, probabilities)

# Nice print out of discrete probability distribution
weights_discrete_prob_distribution %>% gt()

# Graph of discrete probability distribution
ggplot(weights_discrete_prob_distribution, aes(x=weights, y=probabilities)) + 
  geom_col(fill = "steelblue", color = "black") + # Use geom_col() for bar plots of pre-summarized data
  labs(title = "Discrete Probability Distribution of Penguin Weights",
       x = "Weight (g)",
       y = "Probability") +
  theme_minimal() + # Cleaner theme
  theme(plot.title = element_text(hjust = 0.5)) # Center the plot title

####Binomial Probability Distribution  for Species

#Find the probability a penguin is species Adelie (one trial)
p <- sum(df$species == "Adelie")/ length(df$species)

#Find the probability a penguin is not species Adelie (one trial)
q <- sum(df$species != "Adelie")/ length(df$species)

#Check that  p + q = 1
p + q

#Find binomial probabilities that if n penguins are selected at random,
# exactly x of them will be Adelie species

x <- 0:10
n <- 10
binomial_probabilities <- round(dbinom(x,n,p),4)

#Construct binomial probability distribution
species_binomial_prob_distribution <- data.frame(x, binomial_probabilities) 

#Nice print our of binomial probability distribution
species_binomial_prob_distribution %>% gt() 

breaks <- c(0:n)
ggplot(species_binomial_prob_distribution, aes(x, y=binomial_probabilities, fill="red")) + geom_bar(stat = "identity") + scale_x_continuous(breaks = breaks) + ggtitle("Binomial Probability Distribution for Species Adelie")
```

## Interpretation of Binomial Probability Distribution

-   The histogram visualizes the data from the table, with the x-axis representing the number of successes and the y-axis representing the probability of that number of successes.\
-   Each bar's height corresponds to the probability of that number of successes occurring. For instance, the tallest bar at x=4 corresponds to the highest probability (about 0.2420) among the probabilities listed in the table, meaning that observing exactly 4 successes is the most probable outcome within this dataset.\
-   This visualization is particularly useful in showing the distribution of probabilities across different numbers of successes. It provides a quick way to see which outcomes are most and least likely and how the probabilities are distributed (e.g., if they are skewed towards lower or higher numbers of successes).\

```{r}
# Cleaner density plot visualization in place of discrete probability distribution for weights
ggplot(df, aes(x = weight)) + 
  geom_density(fill = "blue", alpha = 0.5) +
  scale_y_continuous(labels = scales::comma) +
  xlab("Weight (g)") + 
  ylab("Density") +
  ggtitle("Density Plot of Penguin Body Weight in Grams")
```

```{r}
### Continuous Probability Distribution for Weights

# Construct a Q-Q plot for body_mass_g
qq_plot <- ggplot(df, aes(sample = weight)) +
  stat_qq(color = "red") +
  stat_qq_line() +
  ggtitle('Q-Q Plot of Penguin Body Mass') +
  xlab('Theoretical Quantiles') +
  ylab('Sample Quantiles')

# Display the Q-Q plot
print(qq_plot)
```

## Q-Q Plot interpretation

I created a discrete probability distribution and density curve with superimposed normal distribution for the weights of penguins in the PalmerPenguins dataset. The mean weight in the data is approximately equal to 4202 grams, with P(4202) approximately equal to 3.5%. The probabilities and weights are not normally distributed. The Q-Q plot confirms this.

```{r}
# Calculate mean and standard deviation of body_mass_g
body_mass_stats <- df %>%
  summarise(mean_mass = mean(weight, na.rm = TRUE),
            sd_mass = sd(weight, na.rm = TRUE))

# Extract the mean and standard deviation
mean_mass <- body_mass_stats$mean_mass
sd_mass <- body_mass_stats$sd_mass

# Density plot with superimposed normal distribution
ggplot(df, aes(x = weight)) + 
  geom_density(fill = "blue", alpha = 0.5) +
  stat_function(fun = dnorm, args = list(mean = mean_mass, sd = sd_mass), color = "red", linetype = "dashed") +
  scale_y_continuous(labels = scales::comma) +
  xlab("Weight (g)") + 
  ylab("Density") +
  ggtitle("Density Plot of Weight in Grams with Superimposed Normal Distribution")
```

```{r}
#Normal Approximation to Binomial Distribution for Species

# Find the probability a penguin is species Adelie (one trial)
p <- sum(df$species == "Adelie") / length(df$species)

# Find the probability a penguin is not species Adelie (one trial)
q <- sum(df$species != "Adelie") / length(df$species)

# Check that p + q = 1
p + q

# Find binomial probabilities that if n penguins are selected at random,
# exactly x of them will be Adelie, where np >= 10 and nq >= 10.
n <- 30
x <- 0:n
np <- n * p
nq <- n * q
cat('np =', np, '>= 10')
cat('nq =', nq, '>= 10')

binomial_probabilities <- round(dbinom(x, n, p), 4)

# Construct binomial probability distribution
species_binomial_prob_distribution <- data.frame(x, binomial_probabilities)

# Superimpose a normal distribution
mean_binom <- n * p
sd_binom <- sqrt(n * p * q)

ggplot(species_binomial_prob_distribution, aes(x, y = binomial_probabilities)) + 
  geom_bar(stat = "identity", fill = "blue") +  # Make it blue
  stat_function(fun = dnorm, args = list(mean = mean_binom, sd = sd_binom), color = "red") +
  scale_x_continuous(breaks = 0:n) +
  ggtitle("Binomial Probability Distribution for Species Adelie with Normal Approximation")
```

```{r}
# Find binomial probabilities that if n penguins are selected at random,
# exactly x of them will be Adelie species
x <- 0:10
n <- 10
binomial_probabilities <- round(dbinom(x, n, p), 4)

# Construct binomial probability distribution
species_binomial_prob_distribution <- data.frame(x, binomial_probabilities)


breaks <- c(0:n)
mean_binom <- n * p
sd_binom <- sqrt(n * p * q)

ggplot(species_binomial_prob_distribution, aes(x, y = binomial_probabilities)) + 
  geom_bar(stat = "identity", fill = "blue") + 
  stat_function(fun = dnorm, args = list(mean = mean_binom, sd = sd_binom), color = "red") +
  scale_x_continuous(breaks = breaks) +
  ggtitle("Binomial Probability Distribution for Species Adelie with Normal Approximation")
```

## Binomial Probability Plot interpretation

-   The histogram visualizes the data from the table, with the x-axis representing the number of successes and the y-axis representing the probability of that number of successes.\
-   Each bar's height corresponds to the probability of that number of successes occurring. For instance, the tallest bar at x=13 corresponds to the highest probability (about 0.14) among the probabilities listed in the table, meaning that observing exactly 13 successes is the most probable outcome within this dataset.\
-   This visualization is particularly useful in showing the distribution of probabilities across different numbers of successes. It provides a quick way to see which outcomes are most and least likely and how the probabilities are distributed (e.g., if they are skewed towards lower or higher numbers of successes).\

```{r}
#Sampling Distribution of Sample Means for weight 

set.seed(666) # for reproducibility

mu <- mean(df$weight, na.rm = TRUE) #population mean
sigma <- sd(df$weight, na.rm = TRUE) #population standard deviation

# Number of samples and sample size
msm <- 1000 # number of samples means
nsm <- 30 # sample size means

# Initialize a vector to store the sample means
sample_means <- numeric(msm)

# Loop to draw samples and compute means
for (i in 1:msm) {
  sample <- sample(df$weight, nsm, replace = TRUE)
  sample_means[i] <- mean(sample, na.rm = TRUE)
}

expected_mu_xbar <- mu  #expected mean of sample means
expected_sigma_xbar <- sigma/sqrt(nsm) #expected standard error of sample means

actual_mu_xbar <- mean(sample_means, na.rm = TRUE)  #actual mean of sample means
actual_sigma_xbar <- sd(sample_means, na.rm = TRUE)  #actual standard error of sample means

# Create a dataframe for the sampling distribution
sampling_distribution <- data.frame(sample_means = sample_means)

# Plot the sampling distribution
ggplot(sampling_distribution, aes(x = sample_means)) +
  geom_histogram(aes(y = ..density..), binwidth = 50, color = "black", fill = "blue") +
  geom_density(color = "red", size = 1) +
  ggtitle("Sampling Distribution of Sample Means for Weight (g)") +
  xlab("Sample Mean Weight (g)") +
  ylab("Density")

qqnorm(sample_means, col = "blue")

cat('The population mean weight is:', mu, 'with standard deviation', sigma,'.')
cat('The expected mean of the sample means is:', expected_mu_xbar, 'with standard error', expected_sigma_xbar,'.')
cat('The actual mean of the sample means is:', actual_mu_xbar, 'with standard error', actual_sigma_xbar,'.')

```

## Interpretation of Sampling Distribution with Superimposed Normal Curve

I made a sampling distribution of sample means for samples of size $n=500$ for the weights of the penguins in the PalmerPenguins dataset. The sampling distribution of sample means is approximately normal. This is confirmed by the normal QQ plot.

-   The population mean weight is $\mu = 4201.754$ with a standard deviation of $\sigma = 801.9545$.
-   The expected mean of the sample means is $\mu_{\bar{x}} = \mu = 4201.754$ with standard error $\mu_{\bar{x}} = \frac{\sigma}{\sqrt{n}} = 146.4162$.
-   The actual mean of the sample means is $\mu_{\bar{x}} = 4202.233$ with standard error $\sigma_{\bar{x}} = 143.2586$.
-   These results support the conclusions of the Central Limit Theorem.

```{r}
#Sampling Distribution of Sample Proportions for Species Adelie

p <- sum(df$species == "Adelie")/length(df$species) #population proportion success
q <- 1-p #population proportion failure

nsp <- 500 #sample size proportions
msp <- 10000 #number of samples proportions

sample_positions <- c( )
sample_proportions <- c( )


for(i in 1:msp){
  sample_species <- sample(df$species,nsp,replace = TRUE)
  sample_proportions[i] <- sum(sample_species == "Adelie")/length(sample_species)
}

expected_mu_phat <- p #expected mean of sample proportions
expected_sigma_phat <- sqrt(p*q/nsp) #expected standard error of sample proportions

actual_mu_phat <- mean(sample_proportions) #actual mean of sample proportions
actual_sigma_phat <- sd(sample_proportions) #actual standard error of sample proportions

ggplot() + geom_density(aes(sample_proportions),color = "red") +  
  stat_function(fun = dnorm, n = 101, args = list(mean = actual_mu_phat, sd = actual_sigma_phat),color = "blue") +
  ggtitle("Sampling Distribution of Sample Proportion for Species Adelie")

qqnorm(sample_proportions, col = "blue")

cat('The population proportion of species Adelie is:', p, '.')
cat('The expected mean of the sample proportions is:', expected_mu_phat, 'with standard error', expected_sigma_phat,'.')
cat('The actual mean of the sample proportions is:', actual_mu_phat, 'with standard error', actual_sigma_phat,'.')
```

## Interpretation of Sampling Distribution of Sample Proportions

I made a sampling distribution of sample proportions for samples of size $n = 500$ for the proportion of the players in the PalmerPenguins dataset that are species Adelie.The sampling distribution of sample proportions is approximately normal. This is confirmed by the normal QQ plot.

-   The population proportion of Adelie penguins is $p=0.4418605$.
-   The expected mean of the sample proportions is $\mu_{\hat{p}} = p = 0.4418605$ with standard error $\sigma_{\hat{p}} = 0.4417294$ with standard error $\sigma_{\hat{p}} = 0.02191825$
-   These results are as expected.

# 4. Confidence Intervals and Hypothesis Tests

For the purpose of this step, I assumed that the weights of penguins in my dataset were a population. I took a sample of the penguin weights of size 100 and I used the sample mean height, along with the population mean and the population standard deviation of the player heights to construct a z confidence interval and to conduct a z hypothesis test.

I confirmed my results with the R function z.test from the R package BSDA.

## z Confidence Interval for the Population Mean

My sample yielded the following 95% confidence interval for population weight: (4146 , 4482). The population standard deviation is approximately 802 grams. Therefore, we may conclude with 95% confidence that the population mean penguin weight in the Palmer Penguins data is between 4146 and 4482 grams.

## z Hypothesis Test for the Population Mean

I used a z hypothesis test with a level of significance of $\alpha = 0.05$ and corresponding critical value of $z_c = \pm 1.96$ to test the claim that the population mean penguin weight is equal to 4202 grams.

My sample yielded a test statistic of $z = 0.277138$ and a p-value of $p = 0.7816742$. Therefore, the null hypothesis was not rejected and in turn there was not sufficient statistical evidence to reject the claim.

### Confirmation of Results

The R function z.test yielded the following results:

One-sample z-Test

data: sample_weights z = -0.33673, p-value = 0.7363 alternative hypothesis: true mean is not equal to 4201.754 95 percent confidence interval: 4017.57 4331.93 sample estimates: mean of x 4174.75

```{r}
library(BSDA)
library(tidyverse)
library(dplyr)

#Find population mean and standard deviation
mu <- mean(df$weight, na.rm = TRUE) #population mean 
sigma <- sd(df$weight, na.rm = TRUE) #population sd

cat('The population mean penguin weight is:',mu,'with population standard deviation',sigma)

#Take random sample of size greater than 30 with replacement. 
n <- 100
sample_weights <- sample(df$weight, n, replace = TRUE) # Added replace = TRUE for sampling with replacement

#CONFIDENCE INTERVAL

#Construct 95% CI for weights based on sample
xbar <- mean(sample_weights, na.rm = TRUE) #sample mean
s <- sd(sample_weights, na.rm = TRUE) #sample sd
zc <- qnorm(0.975) # Corrected to only calculate once for the upper tail
se <- s/sqrt(n) # Changed to use sample standard deviation 's' instead of population 'sigma'
E <- zc*se
lower_bound_CI <- xbar - E
upper_bound_CI <- xbar + E

cat('95% confidence interval for population mean: (',lower_bound_CI,',',upper_bound_CI,')') 

#TWO TAILED HYPOTHESIS TEST
#Step 1: HO: mu = mean(weights),  HA: mu != mean(weights)
claim <- "H0"


#Step 2: 
alpha <- 0.05
zc <- qnorm(1 - alpha/2) # Corrected to find the critical value for both tails

#Step 3: 
xbar <- mean(sample_weights, na.rm = TRUE)
mu_xbar = mu
se <- sigma/sqrt(n) 
z = (xbar-mu_xbar)/se
if (!is.na(z)) { # Added check to ensure z is not NA before proceeding
  if (z > 0) {
    p <- (1- pnorm(z,0,1))*2
  } else {
    p <- pnorm(z,0,1)*2 
  }
  z
  p
  
  #Step 4: 
  if (p > 0.05){
    cat('Since p > 0.05: do not reject H0.')
    rejectH0 <- FALSE
  } else {
    cat('Since p < 0.05: reject H0.') 
    rejectH0 <- TRUE
  }
  
  #Step 5
  if (claim == "H0" & rejectH0 == TRUE){
    cat('There is enough evidence to reject the claim.')
  } else if (claim == "H0" & rejectH0 == FALSE) {
    cat('There is not enough evidence to reject the claim.')
  } else if (claim == "HA" & rejectH0 == TRUE) {
    cat('There is enough evidence to support the claim.')
  } else {
    cat('There is not enough evidence to support the claim.')
  }
}

#Confirm CI and hypothesis test with z.test function

z.test(sample_weights, mu = mu, sigma.x = sigma, conf.level = 0.95)  
```

```{r}
# Calculate sample mean and standard deviation
xbar <- mean(df$weight, na.rm = TRUE) # Sample mean
s <- sd(df$weight, na.rm = TRUE) # Sample standard deviation
n <- nrow(df) # Sample size

cat('The sample mean penguin weight is:',xbar,'with sample standard deviation',s) 


#CONFIDENCE INTERVAL

#Construct 95% CI for weights based on your sample
tc <- qt(0.975,n-1) 
se <- s/sqrt(n) 
E <- tc*se
lower_bound_CI <- xbar - E
upper_bound_CI <- xbar + E

cat('95% confidence interval for population mean: (',lower_bound_CI,',',upper_bound_CI,')') 


# Bootstrap estimation for population mean weight in grams

# 1. Ensure the 'weight' column is numeric, in case it's not
df$weight <- as.numeric(as.character(df$weight))

# 2. Bootstrap Function
bootstrap_mean <- function(df, n_bootstrap) {
  bootstrap_means <- numeric(n_bootstrap)
  for (i in 1:n_bootstrap) {
    sample_indices <- sample(1:length(df), replace = TRUE)
    sample_data <- df[sample_indices]
    bootstrap_means[i] <- mean(sample_data, na.rm = TRUE)
  }
  return(bootstrap_means)
}

# 3. Generate Bootstrap Distribution
# Assuming we want to perform 1000 bootstrap resamples
n_bootstrap <- 15000
bootstrap_distribution <- bootstrap_mean(df$weight, n_bootstrap)

# 4. Estimate Mean and Confidence Interval
estimated_mean <- mean(bootstrap_distribution)
conf_interval <- quantile(bootstrap_distribution, probs = c(0.025, 0.975))

# Print the results
print(paste("Estimated Mean:", estimated_mean))
print(paste("95% Confidence Interval:", conf_interval[1], "-", conf_interval[2]))

# TWO TAILED HYPOTHESIS TEST

# Define hypotheses
# Null hypothesis (HO): mu = 4202
# Alternative hypothesis (HA): mu != 4202

# Step 1: Significance level
alpha <- 0.05

# Step 2: Calculate test statistic
mu_0 <- 4202 # Hypothesized population mean
se <- s / sqrt(n) # Standard error
t <- (xbar - mu_0) / se # Test statistic

# Step 3: Calculate p-value for two-tailed test
p <- 2 * (1 - pt(abs(t), n - 1))

cat('Test statistic:', t, '\n')
cat('P-value:', p, '\n')

# Step 4: Decision rule
if (p > alpha) {
  cat('Since p > alpha: do not reject H0.\n')
  rejectH0 <- FALSE
} else {
  cat('Since p <= alpha: reject H0.\n')
  rejectH0 <- TRUE
}

# Step 5: Conclusion
if (!rejectH0) {
  cat('There is not enough evidence to reject the null hypothesis (HO).\n')
} else {
  cat('There is enough evidence to reject the null hypothesis (HO).\n')
}

#Confirm CI and hypothesis test with z.test function

result <- t.test(df$weight, mu = 4202) #for sample data set
#result <- t.test(sample_heights, mu = 78) #for populations data set

result

#z.test(sample_heights, mu = mu, sigma.x = sigma, conf.level = 0.95)  
```

## Result Interpretation

For the purpose of this step, I assumed that the weights of the penguins in my dataset were a sample. I used the sample mean and standard deviation of the weights to construct a t confidence interval and to conduct a t hypothesis test. I also used bootstrap estimation for the population mean, since a Google search for this information yielded no useful insights.

I confirmed my results with the R function t.test.

### t Confidence Interval for the Population Mean

My sample yielded the following 95% confidence interval for population mean: (4116.7, 4286.8). Therefore, we may conclude with 95% confidence that the population mean penguin weight is between 4116.7 and 4286.8 grams.

### t Hypothesis Test for the Population Mean

I used a t hypothesis test with a significance level of $\alpha = 0.05$ and corresponding critical value of $t_c = \pm 1.96$ to test the claim that the population mean penguin weight is equal to 4202 grams. My sample yielded a test statistic of $t = -0.005$ and a p-value of $p = 0.995471$. Therefore, the null hypothesis was not rejected due to insufficient evidence to reject the claim.

### Confirmation of Results

The R function `t.test` yielded the following results:

One Sample t-test

data: df\$weight t = -0.0056639, df = 341, p-value = 0.9955 alternative hypothesis: true mean is not equal to 4202 95 percent confidence interval: 4116.458 4287.050 sample estimates: mean of x 4201.754

# 5. Analysis of Variance (ANOVA) and Post-Hoc Testing

For the purpose of this step, I assumed the weights of the penguins in this dataset were a population. Three species are represented in the `palmerpenguins` dataset: Adelie, Chinstrap, and Gentoo. This week’s analysis involved ANOVA analysis, followed by Tukey’s Honestly Significant Difference (HSD) in post-hoc testing. This week’s analysis did not include any dataset changes.

## ANOVA Results

Using ANOVA testing requires the condition of homoscedasticity (equal variances), which was not met according to Levene's Test. Three additional methods were applied, including (1) Welch's ANOVA, (2) Box-Cox transformation, and (3) Log transformation. All reached the same conclusion: the null hypothesis should be rejected due to sufficient statistical evidence that weights by species are not equal.

In analyzing the weight variation across different penguin species, the ANOVA analysis provides compelling evidence of significant differences. With an F-value of 343.6, the test yields a p-value of less than 0.001, affirming that the species variable significantly influences weight. The statistical analysis indicates that the species factor is a predominant contributor to the observed variance in weights among penguins, highlighting its importance in the study of their physical characteristics.

Df Sum Sq Mean Sq F value Pr(\>F)\
species 2 146864214 73432107 343.6 \<2e-16 ***Residuals 339 72443483 213698\
--- Signif. codes: 0 ‘***’ 0.001 ‘\*\*’ 0.01 ‘\*’ 0.05 ‘.’ 0.1 ‘ ’ 1 2 observations deleted due to missingness

## Tukey’s HSD Results

The Tukey HSD test results suggest that there is a significant difference in the mean weight between Gentoo penguins and both other species (Adelie and Chinstrap), with Gentoos being heavier on average. However, there is no significant difference in the mean weight between Chinstrap and Adelie penguins. The confidence intervals for the significant differences do not include 0, which supports the finding of a significant difference, while the confidence interval for the non-significant difference does include 0.

Tukey multiple comparisons of means 95% family-wise confidence level

Fit: aov(formula = weight \~ species, data = df)

\$species diff lwr upr p adj Chinstrap-Adelie 32.42598 -126.5002 191.3522 0.8806666 Gentoo-Adelie 1375.35401 1243.1786 1507.5294 0.0000000 Gentoo-Chinstrap 1342.92802 1178.4810 1507.3750 0.0000000

```{r}
library(tidyverse)
library(multcomp)
library(car)  # for Levene's test
library(MASS) # for Box-Cox transformation

# Remove rows with missing values in the variables of interest
penguins_complete <- na.omit(df[, c("species", "weight")])

# Perform Levene's test for equal variances
leveneTest(weight ~ species, data = penguins_complete)

### Levene's test shows that the variances are definitely
### not equal, so a standard ANOVA test should not be used.

###### Alternative 1: Welch's ANOVA
anova_results <- oneway.test(weight ~ species, data = penguins_complete, var.equal = FALSE)

# Output the results of Welch's ANOVA
anova_results

###### Alternative 2: Box-Cox Transformation

# Apply the Box-Cox transformation
bc <- boxcox(weight ~ species, data = penguins_complete)

# Find the lambda that maximizes the log-likelihood
best_lambda <- bc$x[which.max(bc$y)]

# Transform the data using the best lambda
penguins_complete$weight_transformed <- (penguins_complete$weight^best_lambda - 1) / best_lambda

# Diagnostic plot to assess normality of transformed data
hist(penguins_complete$weight_transformed, main = "Histogram of Transformed Body Mass", xlab = "Transformed Body Mass")
qqnorm(penguins_complete$weight_transformed)
qqline(penguins_complete$weight_transformed)

# The plot should be a straight line. It's not.

###### Alternative 3: Log Transformation of weight
# Apply log transformation
penguins_complete$weight_log <- log(penguins_complete$weight)

# We can now proceed with the analysis using the transformed data
# The following example performs a simple linear model (ANOVA) on the transformed data
anova_log <- aov(weight_log ~ species, data = penguins_complete)

# Output the summary of the ANOVA
summary(anova_log)

##### All three alternatives reach the same conclusion: 
##### The null hypothesis that the population means are
##### equal should be rejected.

m <- aov(weight ~ species, df)
summary(m)

# Perform Tukey's HSD test
tukey_results <- TukeyHSD(m, conf.level = 0.95)

# Print the results of Tukey's HSD test
print(tukey_results)
```

# 6. Palmer Penguins Species Proportion Analysis

In this analysis, I hypothesized that the proportion of Adélie penguins within a sample from the palmerpenguins dataset represented the overall population proportion. Initially, I established the population proportion of Adélie penguins as $p = 0.44$. A random sample of 100 penguins was then taken to estimate the sample proportion and to assess the population proportion using both a confidence interval and a hypothesis test.

The sample proportion of Adélie penguins was calculated to be $\hat{p} = 0.58$. Using the exact method of the binom.confint() function from the binom package, a 95% confidence interval was constructed, resulting in an interval of (0.477, 0.678). This interval suggests, with 95% confidence, that the true population proportion of Adélie penguins falls within this range.

A two-tailed hypothesis test was conducted to compare the sample proportion against the hypothesized population proportion of 0.44. The z-test yielded a statistic of $z = 0.0695$ with a corresponding p-value of $p = 0.9446$. Given the significance level of $\alpha = 0.05$, and the p-value being greater than this threshold, there was not enough evidence to reject the null hypothesis. Thus, the analysis does not support a significant deviation from the hypothesized population proportion.

To reinforce these findings, the prop.test function was also applied, resulting in a chi-squared statistic close to zero and a high p-value ($p = 0.9879$), further confirming that the null hypothesis should not be rejected. The 95% confidence interval from the Wald method was slightly narrower, ranging from (0.389, 0.494), but consistent with the previous interval in suggesting that the hypothesized proportion is plausible given the sample data.

Overall, the data does not provide sufficient evidence to refute the hypothesis that the population proportion of Adélie penguins is 0.44. Therefore, we maintain the presumption that around 44% of the penguin population in this dataset are Adélie penguins.

```{r}
library(binom)
library(tidyverse)
library(dplyr)
library(stats)

# Assuming the proportion of Adelie penguins in the dataset 
# is our population proportion
p_population <- mean(df$species == "Adelie")

# Take a random sample of size 100 from the dataset
set.seed(666) # For reproducibility
sample_data <- df[sample(nrow(df), 100), ]

# Calculate the sample proportion of Adelie penguins
p_sample <- mean(sample_data$species == "Adelie")

# Construct a 95% confidence interval for the population proportion
conf_interval <- binom.confint(x = sum(sample_data$species == "Adelie"), 
                               n = 100, 
                               conf.level = 0.95, 
                               methods = "exact")

# Extract lower and upper confidence limits
lower_bound <- conf_interval$lower
upper_bound <- conf_interval$upper

# Conduct a z hypothesis test
z_test <- prop.test(x = sum(sample_data$species == "Adelie"), 
                    n = 100, 
                    p = p_population, 
                    alternative = "two.sided", 
                    conf.level = 0.95)

# Print the results
print(conf_interval)
print(z_test)

cat('The population proportion of Adelie penguins is ',p_population) 
cat('The 95% confidence interval for the population proportion of Adelie penguins is (', 
    lower_bound, ', ', upper_bound, ')', sep = '')

# TWO TAILED HYPOTHESIS TEST

# Step 1: Null and Alternative Hypotheses
# H0: p = 0.44, HA: p != 0.44
population_p <- 0.44 
claim <- "H0"

# Step 2: Define alpha level and critical z value
alpha <- 0.05
zc <- qnorm(1 - alpha/2) # critical value for two-tailed test

# Step 3: Calculate sample proportion and z statistic
phat <- mean(df$species == "Adelie") # sample proportion of Adelie penguins
n <- length(df$species) # sample size
se <- sqrt(population_p * (1 - population_p) / n) # standard error
z <- (phat - population_p) / se # z statistic

# Calculate p-value
p_value <- 2 * (1 - pnorm(abs(z))) # two-tailed p-value

# Print z statistic and p-value
cat("Z-statistic:", z, "\nP-value:", p_value)

# Step 4: Decision rule
rejectH0 <- FALSE
if (p_value < alpha) {
  cat('Since p < alpha: reject H0.\n') 
  rejectH0 <- TRUE
} else {
  cat('Since p >= alpha: do not reject H0.\n')
}

# Step 5: Conclusion based on the claim
if (claim == "H0" && rejectH0 == TRUE) {
  cat('There is enough evidence to reject the claim.\n')
} else if (claim == "H0" && rejectH0 == FALSE) {
  cat('There is not enough evidence to reject the claim.\n')
} else if (claim == "HA" && rejectH0 == TRUE) {
  cat('There is enough evidence to support the claim.\n')
} else {
  cat('There is not enough evidence to support the claim.\n')
}

# Confirm CI with binom.confint
num_adelie <- sum(df$species == "Adelie")

# Asymptotic (Wald) confidence interval
conf_int <- binom.confint(x = num_adelie, n = n, method = "asymptotic", conf.level = 1 - alpha)

# Print the confidence interval
cat('The 95% confidence interval for the population proportion of Adelie penguins is (', 
    conf_int$lower, ', ', conf_int$upper, ')\n', sep = '')

# One-sample test for the proportion of Adélie penguins
# Testing against the null hypothesis that the proportion is 0.44
num_adelie <- sum(df$species == "Adelie")
n <- length(df$species) # Total number of observations in the sample

# Perform the test
one_prop_test <- prop.test(x = num_adelie, n = n, p = population_p, alternative = "two.sided", conf.level = 0.95)

# Print the results
print(one_prop_test)
```

# 7. Analysis of Penguin Species Distribution and Island Association

## Chi-Square Goodness of Fit Test

The first analysis employed a Chi-Square Goodness of Fit Test to assess whether the observed distribution of penguin species aligns with an expected uniform distribution. In other words, the test evaluated the null hypothesis that each of the three species (Adélie, Chinstrap, and Gentoo) is equally represented in the dataset. The observed frequencies of each species were tallied, and expected frequencies were calculated assuming an equal distribution across the three categories. The resulting chi-squared statistic was $X^2=31.907$ with $2$ degrees of freedom, leading to a p-value of ${1.179}^{e-07}$. This extremely low p-value indicates strong evidence against the null hypothesis. Therefore, we reject the notion of a uniform distribution of species within the dataset.

## Chi-Square Test for Independence

The second analysis involved a Chi-Squared Test of Independence to investigate a potential association between penguin species and the island they inhabit. The null hypothesis for this test posits that species and island are independent variables, meaning no relationship exists between the two. A contingency table was constructed to summarize the co-occurrence of species and island. The subsequent chi-squared test yielded a chi-squared statistic of $X^2=299.55$ with $4$ degrees of freedom and a p-value less than ${2.2}^{e-16}$. Similar to the previous test, the exceptionally low p-value provides compelling evidence to reject the null hypothesis. Consequently, we conclude that there is a statistically significant association between penguin species and their island of origin.

## Conclusion

The analyses conducted demonstrate that the distribution of penguin species within the dataset is not uniform and that there is a significant association between species and island habitat. Further investigations could explore the nature of this association, such as identifying which species are prevalent on specific islands and examining potential ecological factors influencing these patterns.

```{r}
###### Chi square goodness of fit test
### to test the distribution of species in the penguins dataset

# Set seed for reproducibility
set.seed(666)

# Observe actual frequencies for each species
actual_frequencies <- table(df$species)

# Assuming we expect the species to be evenly distributed
expected_frequencies <- rep(1/length(unique(df$species)), length(unique(df$species)))

# Perform chi-square goodness of fit test
chi_square_test <- chisq.test(x = actual_frequencies, p = expected_frequencies)

# Output the result of the chi-square test
chi_square_test

# Chi-squared test for given probabilities
# 
# data:  actual_frequencies
# X-squared = 31.907, df = 2, p-value = 1.179e-07


##### Chi-square test for independence
### between `species` and `island`

# Test the independence between 'species' and 'island' in the penguins dataset
# Create a contingency table of the two categorical variables
contingency_table <- table(df$species, df$island)

# Perform chi-square test of independence
chi_square_independence_test <- chisq.test(contingency_table)

# Output the result of the chi-square test of independence
chi_square_independence_test

# Pearson's Chi-squared test
# 
# data:  contingency_table
# X-squared = 299.55, df = 4, p-value < 2.2e-16
```

## 8. Multinomial Logistic Regression for Species Classification

The application of multinomial logistic regression to the Palmer Penguins dataset has provided robust insights into the species classification challenges. Leveraging four physical characteristics—culmen length and depth, flipper length, and weight—as predictors, the model demonstrates substantial efficacy in distinguishing between the three species: Adélie, Chinstrap, and Gentoo. The results indicate a perfect classification accuracy of 100% on the training dataset, with all penguins accurately categorized into their respective species. This outcome is further substantiated by the confusion matrix, which revealed zero misclassifications across species, and the sensitivity, specificity, and positive predictive values, which all achieved a score of 1, underscoring the model's impeccable performance on the dataset used.

### Discussion

While the results from the training data are promising, they also raise critical considerations for the model’s application and interpretation:

· **Overfitting Concerns**: The perfect accuracy might suggest that the model has overfitted the training data. This phenomenon occurs when a model learns the details and noise in the training data to an extent that it negatively impacts the performance of the model on new data. Implementing cross-validation techniques or partitioning the data into a separate training and testing set can help evaluate the model's generalizability.

· **Data Imbalance**: If the dataset is imbalanced, with uneven representation of species, it could bias the model towards the majority class. Exploring methods such as synthetic data generation through SMOTE, or adjusting class weights in the model, could help address this potential issue.

· **Feature Engineering**: Currently, the model utilizes straightforward measurements available within the dataset. Investigating more complex features or interactions between features might enhance the model’s ability to generalize better to new, unseen data.

· **Comparative Analysis with Other Models**: Comparing the multinomial logistic regression model’s performance with other classification techniques like decision trees, random forests, or support vector machines could provide deeper insights into the most effective modeling approach for this particular ecological data.

The multinomial logistic regression model’s current application to the Palmer Penguins dataset successfully demonstrates its capability in accurately classifying penguin species based on morphological data. However, the considerations of overfitting, potential data imbalances, and the need for a more robust evaluation via cross-validation highlight the importance of further methodological rigor. This approach not only validates the model's effectiveness but also ensures its adaptability and accuracy in broader ecological and conservation contexts. Future research directions could involve integrating additional ecological variables, applying model regularization techniques, and testing the model on independent datasets to solidify its predictive power and practical utility in biodiversity conservation efforts.

```{r}
# Remove NA values
df %>% drop_na()

# Fit the multinomial logistic regression model
multinom_model <- multinom(species ~ culmen_length_mm + culmen_depth_mm + flipper_length_mm + weight, data = df)

# Print the model summary
summary(multinom_model)

# Call:
#   multinom(formula = species ~ culmen_length_mm + culmen_depth_mm + 
#              flipper_length_mm + weight, data = df)
# 
# Coefficients:
#   (Intercept) culmen_length_mm culmen_depth_mm flipper_length_mm
# Chinstrap  -34.806703         56.16797       -80.32042         -2.546743
# Gentoo      -4.446499         38.97704       -87.02802         -1.281737
# weight
# Chinstrap -0.12676681
# Gentoo     0.02163804
# 
# Std. Errors:
#   (Intercept) culmen_length_mm culmen_depth_mm flipper_length_mm
# Chinstrap 3.330608e+00     53.187991536    3.848396e+01      11.539675662
# Gentoo    3.951616e-05      0.001697574    7.008909e-04       0.007754504
# weight
# Chinstrap 0.2522008
# Gentoo    0.1856241
# 
# Residual Deviance: 0.001556389 
# AIC: 20.00156 


# Predict the training data
df$pred <- predict(multinom_model, newdata = df, type = "class")

# Calculate accuracy
confusionMatrix(df$pred, df$species)

# Confusion Matrix and Statistics
# 
# Reference
# Prediction  Adelie Chinstrap Gentoo
# Adelie       151         0      0
# Chinstrap      0        68      0
# Gentoo         0         0    123
# 
# Overall Statistics
# 
# Accuracy : 1          
# 95% CI : (0.9893, 1)
# No Information Rate : 0.4415     
# P-Value [Acc > NIR] : < 2.2e-16  
# 
# Kappa : 1          
# 
# Mcnemar's Test P-Value : NA         
# 
# Statistics by Class:
# 
#                      Class: Adelie Class: Chinstrap Class: Gentoo
# Sensitivity                 1.0000           1.0000        1.0000
# Specificity                 1.0000           1.0000        1.0000
# Pos Pred Value              1.0000           1.0000        1.0000
# Neg Pred Value              1.0000           1.0000        1.0000
# Prevalence                  0.4415           0.1988        0.3596
# Detection Rate              0.4415           0.1988        0.3596
# Detection Prevalence        0.4415           0.1988        0.3596
# Balanced Accuracy           1.0000           1.0000        1.0000

# Write new csv and rds files
```

## 9. Conclusion: Insights and Future Directions in Penguin Research

This comprehensive exploration of the Palmer Penguins dataset through a variety of statistical techniques, including multinomial logistic regression, has provided significant insights into the complex world of Antarctic penguin biodiversity. The research highlights the critical role of morphological data in understanding species differentiation and adaptation within the challenging environments of the Palmer Archipelago. Our use of multinomial logistic regression confirmed the distinct morphological profiles of Adélie, Chinstrap, and Gentoo penguins, achieving perfect classification accuracy in our training dataset. This accuracy, while indicative of the model’s strength, also raises concerns about potential overfitting, emphasizing the need for rigorous validation and testing on independent datasets.

The findings from various statistical analyses not only augment our understanding of penguin species distribution and their ecological niches but also underline the importance of considering ecological dynamics and evolutionary pressures in conservation strategies. Our results indicate a significant correlation between penguin morphological characteristics and their habitat preferences, which are crucial for effective conservation management.

Looking forward, this study sets the stage for further research that should include deeper dives into the ecological implications of morphological diversity, such as dietary specialization and competitive interactions. Integrating longitudinal and broader geographic data could enhance the generalizability of our findings and help predict changes over time, particularly in response to environmental pressures like climate change.

Additionally, expanding our methodological approaches to include newer statistical models and comparison with other classification techniques will refine our predictive capabilities and improve our understanding of species resilience. By embracing a more holistic approach that incorporates both new data and novel analytical techniques, future research can provide more nuanced insights into the adaptive strategies of these penguins, thereby supporting more targeted and effective conservation efforts in a rapidly changing world.
